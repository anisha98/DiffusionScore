{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.16"
    },
    "colab": {
      "name": "Heat values generation for new dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anisha98/DiffusionScore/blob/main/Heat_values_generation_for_new_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yR8rVEYKyHD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4b857d03-539f-4c9a-b85e-0cd8eda63758"
      },
      "source": [
        "!unzip /content/journalwithsnip.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/journalwithsnip.zip\n",
            "  inflating: journalwithsnip.csv     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeLNCaPL3EhB"
      },
      "source": [
        "import math\n",
        "import queue\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z8qIE-03EhF"
      },
      "source": [
        "## Function to find all the adjacent nodes to each node in the network\n",
        "def adjacency_matrix(visited,stack,stack_alpha): \n",
        "    g = data\n",
        "    rows = g.shape[0]\n",
        "    cols = g.shape[1]\n",
        "    for x in range(0, rows):\n",
        "        l = []\n",
        "        for y in range(0, cols):\n",
        "            if(g[x,y] == 1):\n",
        "                l.append(y)\n",
        "        adj.append(l)\n",
        "    #print(adj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5RZBBux3OBk"
      },
      "source": [
        "# Function to define the parent nodes of the current node\n",
        "def parent_create(d, parent):\n",
        "  for j in d:\n",
        "      parent[j] = []\n",
        "      for i in range(len(adj)):\n",
        "          if j in adj[i]:\n",
        "            parent[j].append(i)\n",
        "#parent   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZF8AqM53EhI"
      },
      "source": [
        "## Depth First Search function is used to find the distance of each node from each other in the network\n",
        "def DFS(v,visited,stack,stack_alpha):\n",
        "    visited[v] = 1\n",
        "    stack.append(v)\n",
        "    for i in adj[v]:\n",
        "        if visited[i] == 0:\n",
        "            stack_alpha[i] = stack_alpha[v] + 1.0 # if there is a link between two nodes then the distance is incremented\n",
        "            DFS(i,visited,stack,stack_alpha)\n",
        "    #print(\"Stack_alpha\",stack_alpha)\n",
        "    return stack_alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJNu9ulK3EhL"
      },
      "source": [
        "## Depth First Search function is used to combine all the values of closeness centrality and degree centrality and value of\n",
        "## stack_alpha which is the value of the parent node of the current node\n",
        "def Depth(v,visited,stack,stack_alpha):\n",
        "    visited[v] = 1\n",
        "    stack.append(v)\n",
        "    #print(\"Visited\",v)\n",
        "    #print(\"Stack\",stack)\n",
        "    for i in adj[v]:\n",
        "        #print(\"Current\",i)\n",
        "        if visited[i] == 0:\n",
        "            stack_alpha[i] = (stack_alpha[v]) - d[i,0]/N + c[i,0]/N\n",
        "            Depth(i,visited,stack,stack_alpha)\n",
        "    #print(\"Stack_alpha\",stack_alpha)\n",
        "    return stack_alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw9-rZNi3EhS"
      },
      "source": [
        "def heat_layer(y):\n",
        "    return sum(heat[y])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ26pu-m3EhV"
      },
      "source": [
        "df1 = pd.DataFrame(columns = ['University','Article_ID','Snip','Score','Heat_values','Layers','Hops','Direct_citation','Indirect_citation','Total_Nodes','UScore'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZOsPCQu3EhY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "b314c28d-6f07-489a-ebcb-cca5856810de"
      },
      "source": [
        "data_snip = pd.read_csv('/content/journalwithsnip.csv')\n",
        "snip = data_snip['id'].tolist()\n",
        "data_snip.head()\n",
        "#snip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  ...  year\n",
              "0           0  ...  2013\n",
              "1           1  ...  2013\n",
              "2           2  ...  2013\n",
              "3           3  ...  2013\n",
              "4           4  ...  2012\n",
              "\n",
              "[5 rows x 13 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>doc_type</th>\n",
              "      <th>fos</th>\n",
              "      <th>snip</th>\n",
              "      <th>id</th>\n",
              "      <th>meta</th>\n",
              "      <th>organization</th>\n",
              "      <th>publisher</th>\n",
              "      <th>references</th>\n",
              "      <th>source_and_target</th>\n",
              "      <th>title</th>\n",
              "      <th>venue</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>['computer vision', 'image segmentation', 'nat...</td>\n",
              "      <td>8.08</td>\n",
              "      <td>1969616664</td>\n",
              "      <td>{2562555919: {'organization': ['national unive...</td>\n",
              "      <td>['stony brook university', 'stony brook univer...</td>\n",
              "      <td>IEEE Computer Society</td>\n",
              "      <td>[1528802670, 1584193343, 1601567445, 168784646...</td>\n",
              "      <td>[[2437478628, 2328078142], [2511548893, 232807...</td>\n",
              "      <td>BabyTalk: Understanding and Generating Simple ...</td>\n",
              "      <td>IEEE Transactions on Pattern Analysis and Mach...</td>\n",
              "      <td>2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['statistical classification', 'theoretical co...</td>\n",
              "      <td>4.69</td>\n",
              "      <td>2050386482</td>\n",
              "      <td>{2126544388: {'organization': ['dept. of elect...</td>\n",
              "      <td>['institute of telecommun., vienna university ...</td>\n",
              "      <td>IEEE</td>\n",
              "      <td>[105338397, 1483758791, 1634279436, 1751228448...</td>\n",
              "      <td>[[2725938612, 2510844205], [2510844205, 151358...</td>\n",
              "      <td>Distributed particle filtering in agent networ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>['support vector machine', 'cluster analysis',...</td>\n",
              "      <td>1.62</td>\n",
              "      <td>2077494984</td>\n",
              "      <td>{2745134596: {'organization': ['department of ...</td>\n",
              "      <td>['stony brook university', 'university of hels...</td>\n",
              "      <td>Pergamon Press, Inc.</td>\n",
              "      <td>[1494289067, 1555648035, 1777939696, 179095494...</td>\n",
              "      <td>[[2075568737, 2007979335], [2293377724, 200797...</td>\n",
              "      <td>Live and learn from mistakes: A lightweight sy...</td>\n",
              "      <td>Information Processing and Management</td>\n",
              "      <td>2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>['tropism', 'machine learning', 'phenomenologi...</td>\n",
              "      <td>1.19</td>\n",
              "      <td>2101273298</td>\n",
              "      <td>{2895331146: {'organization': ['virginia commo...</td>\n",
              "      <td>['stony brook university', ',  hebrew universi...</td>\n",
              "      <td>Frontiers Media SA</td>\n",
              "      <td>[1971689790, 2029408292, 2073985768, 207570475...</td>\n",
              "      <td>[[1904086895, 2101273298], [2138674902, 210127...</td>\n",
              "      <td>Self-referential forces are sufficient to expl...</td>\n",
              "      <td>Frontiers in Neuroinformatics</td>\n",
              "      <td>2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>['data mining', 'machine learning', 'artificia...</td>\n",
              "      <td>0.87</td>\n",
              "      <td>2123372882</td>\n",
              "      <td>{2803549185: {'organization': ['marmara univer...</td>\n",
              "      <td>['fzi research center for information technolo...</td>\n",
              "      <td>Taylor &amp; Francis Group</td>\n",
              "      <td>[44713956, 76044956, 1507455930, 1542433116, 1...</td>\n",
              "      <td>[[2213728383, 1500684598], [2736057823, 262690...</td>\n",
              "      <td>REAL-TIME COMPLEX EVENT RECOGNITION AND REASON...</td>\n",
              "      <td>Applied Artificial Intelligence</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQndBLcQ3Ehb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9a894448-a9eb-4135-ff27-e2b28d363230"
      },
      "source": [
        "data_snip[['snip']].min()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "snip    0.26\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu8obmo63Ehd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9eaa67d8-3e59-4cd2-af0c-ab6e9cc1284a"
      },
      "source": [
        "# Step number 1) Read the data from each university and for each adjacency matrix\n",
        "directory = ''\n",
        "names = ['Minnesota/']\n",
        "#names = ['clemenson/','colorado/','missouri/','nevada/','new mexico/','north dakota/','oregon/','south carolina/','stony brook/','temple/','vermont/','west virginia/']\n",
        "for i1 in names:\n",
        "    final = os.listdir(directory+i1)\n",
        "    for i,name in enumerate(final): # Iterate through all the files in directory + names\n",
        "        num = int(name[:-4]) # Splitting the string to find the article id \n",
        "        if num in snip:\n",
        "            univ = i1 # assign vairbale univerisity for the current university data\n",
        "            if(directory+i1+name):\n",
        "                print(i1+name)\n",
        "                data = pd.read_csv(directory+i1+name) #Read data using Pandas\n",
        "                idx = data.index[data['Unnamed: 0'] == num] ## Since the location of the source node in the adjacency matrix is not\n",
        "                ## known we get the index of the source node from the matrix \n",
        "                idx.tolist() # convert the index value into a list for later use(this solves the type problem)\n",
        "                data = data.drop('Unnamed: 0',axis = 1) ## preprocess the data to make it into a numpy array\n",
        "                data = data.to_numpy()\n",
        "                N = len(data[0]) ##Total no of nodes in the matrix \n",
        "                #print(N)\n",
        "                #print(\"source heat\",idx[0])\n",
        "                # Initialise variables \n",
        "                visited = [0]*N # used for DFS\n",
        "                stack = [] # used for DFS\n",
        "                stack_alpha = [0]*N # used for DFS\n",
        "                adj = [] # to store the adjacency matrix returned from the adjacency_matrix function \n",
        "\n",
        "                # function call to adjacency_matrix\n",
        "                adjacency_matrix(visited,stack,stack_alpha)\n",
        "\n",
        "                d1 = [i for i in range(0,N)]\n",
        "                d1.pop(idx[0])\n",
        "                parent = {}\n",
        "                parent_create(d1,parent)\n",
        "                #print(parent)\n",
        "                alpha_matrix = [] # to store the distance of each node from one another\n",
        "                for i in range(N):\n",
        "                    initial_node = i\n",
        "                    stack = []\n",
        "                    stack_alpha = [0.0]*N\n",
        "                    visited = [0]*N\n",
        "                    stack_alpha[i] = 1.0\n",
        "                    p = DFS(i,visited,stack,stack_alpha)\n",
        "                    alpha_matrix.append(p)\n",
        "                for i in range(N):\n",
        "                    for j in range(N):\n",
        "                        if(alpha_matrix[i][j] > 0.0):\n",
        "                            alpha_matrix[i][j] -=1 ## decrementing the distance by one to make counting start from 0\n",
        "                for i in range(N):\n",
        "                    alpha_matrix[i][i] = 0.0\n",
        "                #print(\"Alpha_matrix\",alpha_matrix)\n",
        "                layers = alpha_matrix\n",
        "\n",
        "                ## for closeness centrality - calculated by summing all the values in the rows of the alpha_matrix\n",
        "                c = np.zeros(shape=(N,1))\n",
        "                c = np.zeros(shape=(N,1))\n",
        "                for i in range(N):\n",
        "                    sum_dist = 0\n",
        "                    for j in range(N):\n",
        "                        if(alpha_matrix[i][j] >0):\n",
        "                            sum_dist += 1\n",
        "                    c[i,0] = sum_dist\n",
        "\n",
        "                #print(c)\n",
        "                ## Defines the heat kernal\n",
        "                H = np.zeros(shape=(N,N))\n",
        "                row = H.shape[0]\n",
        "                col = H.shape[1]\n",
        "                d = np.zeros(shape=(N,1)) \n",
        "                din = np.zeros(shape=(N,1))\n",
        "                dout = np.zeros(shape=(N,1))\n",
        "                g = data\n",
        "                #count = -10\n",
        "                for y in range(0,row):\n",
        "                    p = g[y] == np.array([0]*N)\n",
        "                    equal = p.all()\n",
        "                    for x in range(0,col):\n",
        "                        dout[y,0] -= g[y,x] #outgoing nodes - should ideally distribute the heat to the next nodes\n",
        "                        din[y,0] += g[x,y] #ingoing nodes - should ideally increase the heat of each node\n",
        "                    d[y,0] = din[y,0] + dout[y,0]\n",
        "\n",
        "                ## To avoid having null values in the closeness centrality array we replace the 0s with -1 else negate the values \n",
        "                ## for futher calculation purpose\n",
        "                for i in range(N):\n",
        "                    if c[i,0]== 0:\n",
        "                        c[i,0] = 1\n",
        "                #print(d)\n",
        "                # to store the distance of each node from one another\n",
        "                alpha_matrix = []\n",
        "                for i in range(N):\n",
        "                    initial_node = i\n",
        "                    stack = []\n",
        "                    stack_alpha = [0.0]*N\n",
        "                    visited = [0]*N\n",
        "                    stack_alpha[i] = 1.0\n",
        "                    p = Depth(i,visited,stack,stack_alpha)\n",
        "                    alpha_matrix.append(p)\n",
        "                for i in range(N):\n",
        "                    alpha_matrix[i][i] = -1.0 ## decrementing the distance by one to make counting start from 0\n",
        "\n",
        "                #print(alpha_matrix)\n",
        "                g = data\n",
        "                for x in range(0,N):\n",
        "                    for y in range(0,N):  \n",
        "                        if g[y,x] == 1 and alpha_matrix[y][x] != 0:\n",
        "                            H[x,y] = 1/alpha_matrix[y][x] #if there is a link between the two nodes x and y and alpha_matrix is not\n",
        "                            # equal to zero then we inverse the value\n",
        "                        else:\n",
        "                            H[x,y] = -alpha_matrix[y][x]/N ## this is for all the other node combinations and hence we dont want\n",
        "                            ## their influence to affect the calculated values much so we divide by N(No of total nodes in the network)\n",
        "\n",
        "                initial_node = idx[0]\n",
        "                arr = [0.0]*N\n",
        "                arr[initial_node] = float(data_snip.loc[data_snip['id'] == num, 'snip'].iloc[0])\n",
        "                ## to initiate the source node value to be equal to its snip or h-index value\n",
        "                initial_heat = np.array(arr)\n",
        "                ## using heat equation calculating the heat in the network where t = 1\n",
        "                heat = np.dot((np.exp(H)),initial_heat)\n",
        "                #print(\"initial array\",initial_heat)\n",
        "                #print(\"heat initially\",heat)\n",
        "                heat1 = [0.0]*N\n",
        "                ## normalising the values in the range of 2-1\n",
        "                for i in range(len(heat)):\n",
        "                    heat1[i] = ( (heat[i] - heat.min()) / (heat.max() - heat.min()) ) * (2 - 1) + 1\n",
        "\n",
        "                ## normalising the values in the range of 0-1\n",
        "                #print(arr[initial_node])\n",
        "                for i in range(len(heat1)):\n",
        "                    heat1[i] = (heat1[i]/2.0) * arr[initial_node]\n",
        "\n",
        "                for i in range(len(heat)):\n",
        "                    heat[i] = heat1[i]\n",
        "\n",
        "                #print(\"after normalisation\",heat)\n",
        "                t = np.array(layers)\n",
        "                d1 = {} \n",
        "                for i in range(N):\n",
        "                    d1[i] = []\n",
        "\n",
        "                ## Code after from here onwards is to find the heat layer wise in the network\n",
        "                def printLevels(graph, V, x): \n",
        "                  # array to store level of each node  \n",
        "                  level = [[] for i in range(N)]\n",
        "                  # create a queue  \n",
        "                  stk = []\n",
        "                  stk.append(x)\n",
        "                  level[x].append(0)\n",
        "                  marked = [False for i in range(N)]\n",
        "                  while(len(stk) != 0):\n",
        "                      x1 = stk.pop()\n",
        "                      if marked[x1] == False:\n",
        "                        for i in graph[x1]:\n",
        "                            if(i == idx[0]):\n",
        "                              pass\n",
        "                            else:\n",
        "                              stk.append(i)\n",
        "                              #print(i)\n",
        "                              o = parent.get(i)\n",
        "                              for j in o:\n",
        "                                if(level[j] != []):\n",
        "                                  for k in level[j]:\n",
        "                                      level[i].append(k + 1)\n",
        "                                      level[i] = list(set(level[i]))\n",
        "                      marked[x1] = True\n",
        "\n",
        "                  # display all nodes and their levels  \n",
        "                  #print(level)\n",
        "                  for i in range(N):\n",
        "                      #print(level[i])\n",
        "                      d1[i] = level[i]\n",
        "  \n",
        "                # Driver Code  \n",
        "                if __name__ == '__main__': \n",
        "                    \n",
        "                    # adjacency graph for tree  \n",
        "                    V = N\n",
        "                    graph = adj\n",
        "                    printLevels(graph, V, idx[0]) \n",
        "    \n",
        "                ## contains data of which and all layers each node comes in\n",
        "                final_dictionary =  d1 \n",
        "                f_d = {}\n",
        "                max_val = 0\n",
        "                ## calculates the total number of hops in the network\n",
        "                for key,values in final_dictionary.items():\n",
        "                    f_d[key] = list(set(values))\n",
        "                    max_i = max(f_d[key])\n",
        "                    if(max_i > max_val):\n",
        "                        max_val = max_i\n",
        "\n",
        "                ## to calculate all the nodes present in each hops ex: hop1:[node1,node23], hop2:[node0,node12] ...\n",
        "                d_layer = {} \n",
        "                for i in range(max_val+1):\n",
        "                    d_layer[i] = []\n",
        "                for i in range(max_val+1):\n",
        "                    for key,values in f_d.items():\n",
        "                        if i in values:\n",
        "                            d_layer[i].append(key)\n",
        "\n",
        "                ##length of first hop is the direct citation\n",
        "                direct_citation = len(d_layer[1])\n",
        "                o = d_layer.copy()\n",
        "                o.pop(0)\n",
        "                o.pop(1)\n",
        "                indirect_citation = 0\n",
        "                ## sum of length of all the other layers except the first one\n",
        "                for i,values in o.items():\n",
        "                    indirect_citation += len(values)\n",
        "                if(indirect_citation > 1):\n",
        "                    indirect_citation -=1\n",
        "                tot = N\n",
        "                avg_heat_layer = []\n",
        "                d_layer.pop(0) ## this contains the source node we dont want the heat in the source to be there for the calculation\n",
        "                ## of the heat in each layer\n",
        "\n",
        "                ## to calculate heat in each layer\n",
        "                for i in range(1,max_val+1):\n",
        "                    avg_heat_layer.append(heat_layer(d_layer[i])/len(d_layer[i]))\n",
        "\n",
        "                ## sum all the heats in each layer\n",
        "                score = sum(avg_heat_layer)\n",
        "                #print(num,len(d_layer[1]),d_layer[1],direct_citation,max_val)\n",
        "                print(\"We are done entering the value\")\n",
        "                print(\"*\"*100)\n",
        "                ## append the values to a dataframe for saving purpose\n",
        "                df1 = df1.append({'University': univ, 'Article_ID': num,'Snip':float(data_snip.loc[data_snip['id'] == num, 'snip'].iloc[0]),'Score':score,'doc_type':float(data_snip.loc[data_snip['id'] == num, 'doc_type'].iloc[0]),'Heat_values':heat,'Layers':d_layer,'Hops':float(max_val),'Direct_citation':float(direct_citation),'Indirect_citation':float(indirect_citation),'Total_Nodes':float(tot),'UScore':0.0}, ignore_index=True)\n",
        "            else:\n",
        "                continue"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minnesota/2004744336.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2031299022.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2042554576.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2058704078.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2141766442.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/1799314127.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2027355980.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2139253079.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2087980345.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/1998925295.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2038819732.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2055234679.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2114581724.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2118451900.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2045272935.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/1977125063.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2025984714.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/1966955246.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2006550254.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/1980331821.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2086962710.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2152856313.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2007371521.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2084401272.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2034059032.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2125484079.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2153218475.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2140738143.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2018065418.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/1498723517.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2094589769.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n",
            "Minnesota/2167404137.csv\n",
            "We are done entering the value\n",
            "****************************************************************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KHLbZJx3Ehg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e763917a-c36d-4b15-f6bf-002358d9baa6"
      },
      "source": [
        "len(df1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZG4lG3fqt2a"
      },
      "source": [
        "df1.to_csv(\"Minnesota.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILV_xPxxFEnu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}